# Overview
This project demonstrates how to run LLM models ranging from 0.5B to 3B parameters in Colab's free GPU environment, aimed at converting unstructured project descriptions into structured tech stack information. Due to budget constraints, various model sizes were tested, and an optimal range was identified for efficient execution in Colab.
